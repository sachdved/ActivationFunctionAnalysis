\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[preprint]{NeurIPS}
\usepackage{amsmath}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Notes on Analysis of Activation Functions}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Ue-Yu Pen\thanks{No funding :(} \\
 Chicago, IL \\
  \texttt{ueyupen@gmail.com} \\
  \And
  Vedant Sachdeva\thanks{Also unfunded :(} \\
  Chicago, IL 60615 \\
  \texttt{sachdved@gmail.com}
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
Here, we explore the solutions to regression functions, with varying degrees of non-linearity and depth to explore how and when certain correlations are captured by MLPs.
\end{abstract}

\section{Intro}

Typically, multi-layer perceptron (MLP) networks are characterized by two parameters - depth and width. Depth refers to the number of layers in a network, while the width of a given layer typically refers to the number of nodes in said layer. Layers are parametrized by their weights, $W^{(j)}$, where $j$ indicates that we are examining the $j$th layer. $W^{(j)}$ is typically drawn from $\mathbb{R}^{d^{(j)} \times d^{(j+1)}}$, where $d^{(j)}$ is the width of the $j$th layer. Previous works have shown that with non-linear activation functions and MLP architectures, a vast family of functions can be arbitrarily approximated. However, how such functions correlate to the moments of the data being leveraged is not clearly explained. This document will detail how non-linearities uncover the higher order correlations in data. For the purposes of this note, we will focus exclusively on mean-square-error objective functions.

\section{Linear Activation Functions}

In MLPs with linear activation functions, we observe that the output of the $j$th layer is simply given by

\begin{equation}
x^{(j+1)} = W^{(j)}x^{(j)} + b^{(j)}.
\end{equation}

For simplicity, instead of including a bias term, we will append a node to each layer with value, $1$, independent of input. This value will provide an effective bias to each layer. We can then express the output of the $j$th layer from the initial layer by writing:

\begin{equation}
x^{(j+1)} = \prod_{i=1}^{i=j} W^{(i)} x^{(0)}
\end{equation}

Consequently, any network of arbitrary depth and width with linear activation functions can be re-written as a single layered MLP. Thus, when optimizing our objective function, we are simply optimizing:

\begin{equation}
\mathcal{L} = \min_{W} \sum_{i} || y_{i} - W x_{i}||^{2}.
\end{equation}

We optimize this objective function by computing a derivative with respect to each element of $W$ and equating to $0$. This yields a set of simultaneous equations of the form:

\begin{equation}
\frac{\partial\mathcal{L}}{\partial W^{(k)}} = \sum_{i} (-2y_{i} x^{(k)}_{i} + \sum_{m\neq k} W^{m} x_{i}^{m}x_{i}^{k}  + W^{k} x^{k}_{i} x^{k}_{i})
\end{equation}

Thus, the optimal weight vector can be expressed as:

\begin{equation}
W^{k} = \frac{\mathrm{Cov}(Y, X^{k}) - \sum_{m\neq k}\mathrm{Cov}(X^{m}, X^{k})}{\mathrm{Cov}(X^{k}, X^{k})}
\end{equation}

As we can see, one outcome will be that if much of the covariance in $Y$ with $X_k$ can be explained by $X_k$ covarying with another feature in $X$, the net weight will be constrained, but the individual weights can become unbounded. Further, such a model cannot consider further than second order moments in the dataset. 

\section{Quadratic Activation Functions with one layer}

We will expand our activation function forward to consider quadratic terms. We are doing so with the intent to understand how the optimal solution for polynomial non-linearities involves inference of other statistical moments of the data. We will start here, before moving on to Taylor expansions of common activation functions in the neighborhoods of high curvature regions of the activation function. We will begin our analysis of quadratic activation functions on models with a one dimensional input and a one dimensional output. That simplifies to a model of the form

\begin{equation}
y = Wx + \frac{\alpha}{2} (W^2 x^2).
\end{equation}

We have introduced a parameter, $\alpha$, that serves as a weight on the quadratic term in our activation function. 

Analytically solving for the optimal weight, as before, yields an equation of the form

\begin{equation}
\mathcal{L} = \min_{W} \sum_{i} || y_i - Wx_i - \frac{\alpha}{2} W^{2} x_{i}^{2} ||.
\end{equation}

\begin{equation}
\frac{d\mathcal{L}}{dW} = \sum_{i} (- 2 y_{i} x_{i} - 2 \alpha W y_{i} x_{i}^2 + 2 W x_{i}^{2} +3 \alpha W^{2} x_{i}^{3} + \alpha^{2} W^{3} x_{i}^{4})
\end{equation}

The above is a cubic equation, and its roots can be expressed analytically in terms of the expectations of our random variables. Even without solving, we can immediately see that by including a quadratic term in the activation function, we obtain a dependency on the fourth order moment with only one parameter to fit. 


We note that this is in contrast to if we were simply doing quadratic regression, in which case the objective would be:

\begin{equation}
\mathcal{L} = \min_{W_{1}, W_{2}} \sum_{i} || y_{i} - W_{1} x_{i} - \frac{\alpha}{2} W_{2}  x_{i}^{2} ||
\end{equation}

\begin{align}
\frac{d\mathcal{L}}{dW_{1}} =  -2 \mathrm{E}[XY] + 2W_{1} \mathrm{E}[X^{2}] + \alpha W_{2} \mathrm{E}[X^3]\\
\frac{d\mathcal{L}}{dW_{2}} =  -2\alpha \mathrm{E}[X^{2}Y] + \alpha W_{1} \mathrm{E}[X^{3}] + \frac{\alpha^{2}}{2} W_{2} \mathrm{E}[X^4]
\end{align}


Here, we see a similar dependence on fourth order moments, but we now have two parameters to fit. In some sense, we might argue that a quadratic activation function is compressing more moments into fewer parameters. 

\section{Quadratic Activation Functions with Multiple Layers}

We now imagine the setting with a one dimensional input, a single one dimensional hidden layer, and a one dimensional output.

Then, we can write

\begin{align}
x_{(2)} = W_{(1)}x_{(1)} + \frac{\alpha}{2} (W_{(1)} x_{(1)})^{2} \\
\hat{y} = x_{(3)} = W_{(2)}x_{(2)} + \frac{\alpha}{2} (W_{(2)} x_{(2)})^{2} \\
x_{(3)} = W_{(2)}(W_{(1)}x_{(1)} + \frac{\alpha}{2} (W_{(1)} x_{(1)})^{2}) + \frac{\alpha}{2} (W_{(2)} W_{(1)}x_{(1)} + \frac{\alpha}{2} (W_{(1)} x_{(1)})^{2})^{2}
\end{align}


Simplifying and grouping the last line, we obtain:

\begin{align}
x^{3} = W_{(2)}W_{(1)}x_{(1)} + \frac{\alpha {W_{(1)}^{2} W_{(2)}}}{2} x_{(1)}^{2} + \frac{\alpha}{2}(W^{2}_{(2)} W^{2}_{(1)} x_{(1)}^{2})) + \frac{\alpha^{2}}{2} W_{(2)} W^{3}_{(1)}
\end{align}


\section*{References}


{
\small


[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.


[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.


[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}